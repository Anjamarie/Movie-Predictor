{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aac572-156d-4642-a6f4-c078ede3fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie Predictor Model\n",
    "# Anja Gill\n",
    "# Goals were to see if we could predict the rating of a movie based on features such as budget, production company, cast, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2244aa0-c397-4a02-b635-5bc5811e2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tmdbsimple as tmdb\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import ast\n",
    "import joblib\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4391a9-3174-4136-a156-d444f5f4b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: API_KEY is exposed here. For production, use environment variables.\n",
    "tmdb.API_KEY = '7eb2f50ca573c609c0bac8e9f804514d'\n",
    "\n",
    "PAGES_TO_FETCH = 10  # Reduced pages for quick test generation\n",
    "\n",
    "# Define the features that the Streamlit app actually collects from the user.\n",
    "STREAMLIT_INPUT_FEATURES = [\n",
    "    'budget',\n",
    "    'runtime',\n",
    "    'release_year',\n",
    "    'release_month',\n",
    "    'release_dayofweek',\n",
    "]\n",
    "!Data Acquisition and Preprocessing\n",
    "\n",
    "def get_movie_data(movie_id):\n",
    "    \"\"\"Fetches detailed data for a single movie from TMDb.\"\"\"\n",
    "    try:\n",
    "        movie = tmdb.Movies(movie_id)\n",
    "        info = movie.info()\n",
    "        credits = movie.credits()\n",
    "        keywords = movie.keywords()\n",
    "        \n",
    "        director = next((person['name'] for person in credits['crew'] if person['job'] == 'Director'), None)\n",
    "        cast = [actor['name'] for actor in credits['cast'][:5]]\n",
    "        production_companies = [company['name'] for company in info.get('production_companies', [])[:5]]\n",
    "        genres = [genre['name'] for genre in info.get('genres', [])]\n",
    "        movie_keywords = [keyword['name'] for keyword in keywords.get('keywords', [])]\n",
    "        \n",
    "        # Filter for quality data points (non-zero budget/revenue)\n",
    "        if info.get('budget', 0) == 0 or info.get('revenue', 0) == 0:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'id': info['id'],\n",
    "            'title': info['title'],\n",
    "            'release_date': info.get('release_date'),\n",
    "            'budget': info.get('budget'),\n",
    "            'revenue': info.get('revenue'),\n",
    "            'runtime': info.get('runtime'),\n",
    "            'genres': genres,\n",
    "            'cast': cast,\n",
    "            'director': director,\n",
    "            'keywords': movie_keywords,\n",
    "            'production_companies': production_companies\n",
    "        }\n",
    "    except Exception:\n",
    "        # Silently skip movies with API errors\n",
    "        return None\n",
    "\n",
    "# --- Data Fetching (Use this section ONLY if you need to regenerate data) ---\n",
    "# If you already have 'movies_large_dataset.csv', you can skip this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef5d2e-d07c-4d8d-98da-af3e68da79f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for up to 2000 movies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Pages:  77%|████████████████████      | 77/100 [16:02<04:53, 12.75s/it]"
     ]
    }
   ],
   "source": [
    "all_movie_data = []\n",
    "PAGES_TO_FETCH = 100 \n",
    "print(f\"Fetching data for up to {PAGES_TO_FETCH * 20} movies...\")\n",
    "\n",
    "# Note: Due to API limits, fetching data takes time.\n",
    "for page in tqdm(range(1, PAGES_TO_FETCH + 1), desc=\"Fetching Pages\"):\n",
    "    try:\n",
    "        discover = tmdb.Discover()\n",
    "        response = discover.movie(page=page, sort_by='popularity.desc')\n",
    "        page_movie_ids = [movie['id'] for movie in response['results']]\n",
    "        \n",
    "        for movie_id in page_movie_ids:\n",
    "            data = get_movie_data(movie_id)\n",
    "            if data:\n",
    "                all_movie_data.append(data)\n",
    "            time.sleep(0.1) # Respect API rate limits\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error on page {page}: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame(all_movie_data)\n",
    "df.to_csv('movies_large_dataset.csv', index=False)\n",
    "print(f\"\\nDataset saved with {len(df)} movies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d1a07-f0a2-4490-a3f1-19996d406aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "df = pd.read_csv('movies_large_dataset.csv')\n",
    "\n",
    "# Initial Cleaning\n",
    "df['budget'] = df['budget'].replace(0, np.nan)\n",
    "df['revenue'] = df['revenue'].replace(0, np.nan)\n",
    "df.dropna(subset=['budget', 'revenue'], inplace=True)\n",
    "\n",
    "# Feature extraction from date\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "df['release_year'] = df['release_date'].dt.year\n",
    "df['release_month'] = df['release_date'].dt.month\n",
    "df['release_dayofweek'] = df['release_date'].dt.dayofweek\n",
    "\n",
    "# Convert stored strings back into Python lists\n",
    "for col in ['genres', 'cast', 'keywords', 'production_companies']:\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "df['profit'] = df['revenue'] - df['budget']\n",
    "\n",
    "print(f\"\\nCleaned DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd0c38-4087-4377-9e94-b145c73c65a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 10 genres for one-hot encoding\n",
    "top_genres = df['genres'].explode().value_counts().nlargest(10).index\n",
    "\n",
    "for genre in top_genres:\n",
    "    df[f'genre_{genre}'] = df['genres'].apply(lambda x: 1 if genre in x else 0)\n",
    "\n",
    "# Mean Encoding Logic\n",
    "for col in ['director', 'cast', 'production_companies']:\n",
    "    if col != 'director':\n",
    "        # For lists (cast, companies), explode and calculate mean revenue\n",
    "        exploded_df = df.explode(col)\n",
    "        mapping = exploded_df.groupby(col)['revenue'].mean()\n",
    "        df[f'mean_{col}_revenue'] = df[col].apply(\n",
    "            lambda lst: np.mean([mapping.get(item, 0) for item in lst]) if lst else 0\n",
    "        )\n",
    "    else: \n",
    "        # Director is a single value, use map\n",
    "        mapping = df.groupby(col)['revenue'].mean()\n",
    "        df[f'mean_{col}_revenue'] = df[col].map(mapping).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835c2a0-f1a3-4ddc-b23b-bc712a36e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training with Full Feature Set\n",
    "\n",
    "y = df['revenue']\n",
    "\n",
    "# Define ALL features, including the complex mean-encoded ones\n",
    "features_to_drop_full = [\n",
    "    'id', 'title', 'release_date', 'genres', 'cast', 'director', \n",
    "    'keywords', 'production_companies', 'revenue', 'profit', 'roi'\n",
    "]\n",
    "X_full = df.drop(columns=features_to_drop_full, errors='ignore')\n",
    "X_full.dropna(inplace=True) \n",
    "\n",
    "# Re-align Y after dropping NaNs in X_full\n",
    "y_full = y[X_full.index]\n",
    "\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=11\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining on {X_train_full.shape[1]} features.\")\n",
    "# Example GBR model for initial metrics\n",
    "gbr_full = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=11)\n",
    "gbr_full.fit(X_train_full, y_train_full)\n",
    "\n",
    "y_pred_full = gbr_full.predict(X_test_full)\n",
    "r2_full = r2_score(y_test_full, y_pred_full)\n",
    "\n",
    "print(f\"**Full Feature Model R² Score (Baseline): {r2_full:.4f}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e5cca-e51d-4419-ba32-cb0db61d0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning (GridSearch)\n",
    "\n",
    "print(\"\\n--- Running Grid Search for Best Model Parameters ---\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "grid_search_full = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(random_state=11),\n",
    "    param_grid=param_grid,\n",
    "    cv=3, # Reduced CV for quicker demonstration\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search_full.fit(X_train_full, y_train_full)\n",
    "\n",
    "best_params_full = grid_search_full.best_params_\n",
    "best_model_full = grid_search_full.best_estimator_\n",
    "\n",
    "print(f\"Best Parameters Found: {best_params_full}\")\n",
    "print(f\"Best Cross-validation R² Score: {grid_search_full.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eea3a8-1f64-4e1f-aa28-31542632215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Feature Importance (Showcasing Results)\n",
    "\n",
    "y_pred_tuned_full = best_model_full.predict(X_test_full)\n",
    "rmse_tuned_full = np.sqrt(mean_squared_error(y_test_full, y_pred_tuned_full))\n",
    "r2_tuned_full = r2_score(y_test_full, y_pred_tuned_full)\n",
    "\n",
    "print(\"\\n**Final Full-Feature Model Evaluation**\")\n",
    "print(f\"RMSE: ${rmse_tuned_full:,.2f}\")\n",
    "print(f\"R-squared (R²): {r2_tuned_full:.4f}\")\n",
    "\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X_full.columns,\n",
    "    'importance': best_model_full.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importances.head(10).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bce4c7-1382-4dce-b0bf-a6a140f4643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The highly accurate model above uses features (mean_revenue_...) that cannot \n",
    "# be collected from a live user in the Streamlit app.\n",
    "# We create a new, simplified model that only uses user-collectable features (8 features).\n",
    "\n",
    "print(\"\\n--- Creating Simplified Model for Deployment ---\")\n",
    "\n",
    "# Define the features that the Streamlit app actually collects from the user.\n",
    "STREAMLIT_FEATURES = [\n",
    "    'budget', \n",
    "    'runtime', \n",
    "    'release_year', \n",
    "    'release_month', \n",
    "    'release_dayofweek'\n",
    "]\n",
    "# Add the genre columns created in Step 3\n",
    "STREAMLIT_FEATURES.extend([col for col in X_full.columns if col.startswith('genre_')])\n",
    "\n",
    "# Define X and y for the simplified model\n",
    "X_simple = X_full[STREAMLIT_FEATURES]\n",
    "y_simple = y_full \n",
    "\n",
    "# Re-train the GBR model using the best parameters, but only on the simple features\n",
    "gbr_final_for_app = GradientBoostingRegressor(**best_params_full, random_state=11)\n",
    "gbr_final_for_app.fit(X_simple, y_simple)\n",
    "\n",
    "# Final evaluation of the simplified model (for documentation)\n",
    "y_pred_simple = gbr_final_for_app.predict(X_test_full[STREAMLIT_FEATURES])\n",
    "r2_simple = r2_score(y_test_full, y_pred_simple)\n",
    "\n",
    "print(f\"Simplified Model R² Score: {r2_simple:.4f}\")\n",
    "print(\"--- Final Model Ready for Streamlit ---\")\n",
    "\n",
    "# --- 8. Saving Final Files for Deployment ---\n",
    "\n",
    "# Save the model trained on the simplified features\n",
    "joblib.dump(gbr_final_for_app, 'movie_revenue_model.pkl')\n",
    "\n",
    "# Save only the list of simple feature names (CRUCIAL for app input matching)\n",
    "joblib.dump(STREAMLIT_FEATURES, 'model_features.pkl')\n",
    "\n",
    "print(\"\\nSUCCESS: Final 'movie_revenue_model.pkl' and 'model_features.pkl' saved locally.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17146a-89a7-47a7-a7e8-b104699e2f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
